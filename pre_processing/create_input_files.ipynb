{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d20d24e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from Bio import SeqIO\n",
    "import sys\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import re\n",
    "import scipy\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from pandarallel import pandarallel\n",
    "from scipy.stats import multinomial, chisquare, power_divergence\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efe9876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.multitest import multipletests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f1dcfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "import suffix_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d8b64ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 1 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "pandarallel.initialize(progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f1e27d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_trans = str.maketrans(\"ACGTMRWSYKVHDBN\", \"TGCAKYWSRMBDHVN\")\n",
    "def reverse_complement(seq):\n",
    "    return seq.translate(comp_trans)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "952b44c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_kmers(l=3):\n",
    "    return [\"\".join(tup) for tup in itertools.product(\"ATGC\", repeat=l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c9d3e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kmer_frequencies(seq, k=3):\n",
    "    seq_ = str(seq)\n",
    "    all_kmers = get_all_kmers(k)\n",
    "    kmer_counts = {kmer:0 for kmer in all_kmers}\n",
    "    for i in range(len(seq)-k):\n",
    "        kmer_counts[seq_[i:i+k]] += 1\n",
    "        kmer_counts[reverse_complement(seq_[i:i+k])] += 1\n",
    "    d = pd.DataFrame(list(kmer_counts.items()), columns=['kmer', 'count']).set_index('kmer')\n",
    "    d = d / d.sum()\n",
    "    return d.loc[all_kmers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15fff290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmers(row, n=7, k=3):\n",
    "    for start in range(n):\n",
    "        kmer = row['s'][start:start+k]\n",
    "        if len(kmer) == k:\n",
    "            row[kmer] = 1\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc2616e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_positions(fwd, rev, thr, only_CA=False, peak_dist=0, min_cov=1, min_dist=1, dist_rem='all', subsample=1., k=3, k_window=2, kmer_quantile=1., iterative_kmer_selection=False):\n",
    "    # select based on threshold\n",
    "    fwd_pos, rev_pos = np.where(fwd <= thr)[0], np.where(rev <= thr)[0]\n",
    "    #peak selection\n",
    "    if peak_dist > 0:\n",
    "        for i,pos in enumerate([fwd_pos, rev_pos]):\n",
    "            dists = pos[1:] - pos[:-1]\n",
    "            rem_flank = 'left'\n",
    "            while np.any(dists <= peak_dist):\n",
    "                dists = np.pad(dists, 1, constant_values=[min_dist+1, min_dist+1])#[1,0])\n",
    "                diff = np.diff((dists <= peak_dist).astype(int))\n",
    "                clus = np.column_stack((np.where(diff == 1)[0], np.where(diff == -1)[0]))\n",
    "                # iteratively remove left or right flank of clusters in alternating fashion\n",
    "                if rem_flank == 'left':\n",
    "                    rem = clus[:,0]\n",
    "                    rem_flank = 'right'\n",
    "                else:\n",
    "                    rem = clus[:,1]\n",
    "                    rem_flank = 'left'\n",
    "                pos = np.delete(pos, rem)\n",
    "                dists = pos[1:] - pos[:-1]\n",
    "            if i == 0:\n",
    "                fwd_pos = pos\n",
    "            else:\n",
    "                rev_pos = pos\n",
    "    # require minimum coverage\n",
    "    if min_cov > 1:\n",
    "        fwd_pos = fwd_pos[fwd_cov[fwd_pos] >= min_cov]\n",
    "        rev_pos = rev_pos[rev_cov[rev_pos] >= min_cov]\n",
    "    # kmer-based filtering\n",
    "    if kmer_quantile < 1.:\n",
    "        if iterative_kmer_selection:\n",
    "            n = 2*(k_window+k-1)+1-(k-1)\n",
    "            all_kmers = get_all_kmers(k)\n",
    "            n_most_frequent = int(kmer_quantile * len(all_kmers))\n",
    "            frequent_kmers = []\n",
    "            fwd_pos_ = fwd_pos.copy()\n",
    "            rev_pos_ = rev_pos.copy()\n",
    "            for i in range(n_most_frequent):\n",
    "                d_fwd = pd.DataFrame(fwd_pos_, columns=['pos'])\n",
    "                d_fwd['s'] = d_fwd.apply(lambda row: \"\".join(seq[row.pos - (k_window+k-1): row.pos + (k_window+k)]), axis=1).to_frame()\n",
    "                d_fwd = d_fwd.reindex(d_fwd.columns.tolist() + all_kmers, axis=1, fill_value=0)\n",
    "                d_fwd = d_fwd.parallel_apply(kmers, n=n, k=k, axis=1).set_index('pos').drop('s', axis=1)\n",
    "                d_rev = pd.DataFrame(rev_pos_, columns=['pos'])\n",
    "                d_rev['s'] = d_rev.apply(lambda row: \"\".join(seq[row.pos - (2+k-1): row.pos + (2+k)]), axis=1).to_frame()\n",
    "                d_rev = d_rev.reindex(d_rev.columns.tolist() + all_kmers, axis=1, fill_value=0)\n",
    "                d_rev = d_rev.parallel_apply(kmers, n=n, k=k, axis=1).set_index('pos').drop('s', axis=1)\n",
    "                d = d_fwd.sum() + d_rev.sum()\n",
    "                d = d - (kmer_frequencies[k].loc[d.index]['count'] * d.sum()).astype(int)\n",
    "                frequent_kmers.append(d.idxmax())\n",
    "                fwd_pos_ = d_fwd[d_fwd[[frequent_kmers[-1]]].sum(axis=1) == 0].index.to_numpy()\n",
    "                rev_pos_ = d_rev[d_rev[[frequent_kmers[-1]]].sum(axis=1) == 0].index.to_numpy()\n",
    "            d_fwd = pd.DataFrame(fwd_pos, columns=['pos'])\n",
    "            d_fwd['s'] = d_fwd.apply(lambda row: \"\".join(seq[row.pos - (k_window+k-1): row.pos + (k_window+k)]), axis=1).to_frame()\n",
    "            d_fwd = d_fwd.reindex(d_fwd.columns.tolist() + all_kmers, axis=1, fill_value=0)\n",
    "            d_fwd = d_fwd.parallel_apply(kmers, n=n, k=k, axis=1).set_index('pos').drop('s', axis=1)\n",
    "            d_rev = pd.DataFrame(rev_pos, columns=['pos'])\n",
    "            d_rev['s'] = d_rev.apply(lambda row: \"\".join(seq[row.pos - (2+k-1): row.pos + (2+k)]), axis=1).to_frame()\n",
    "            d_rev = d_rev.reindex(d_rev.columns.tolist() + all_kmers, axis=1, fill_value=0)\n",
    "            d_rev = d_rev.parallel_apply(kmers, n=n, k=k, axis=1).set_index('pos').drop('s', axis=1)\n",
    "            d = d_fwd.sum() + d_rev.sum()\n",
    "            d = d - (kmer_frequencies[k].loc[d.index]['count'] * d.sum()).astype(int)\n",
    "            #print(d[frequent_kmers].sort_values())\n",
    "            fwd_pos = d_fwd[d_fwd[frequent_kmers].sum(axis=1) > 0].index.to_numpy()\n",
    "            rev_pos = d_rev[d_rev[frequent_kmers].sum(axis=1) > 0].index.to_numpy()\n",
    "        else:\n",
    "            n = 2*(k_window+k-1)+1-(k-1)\n",
    "            all_kmers = get_all_kmers(k)\n",
    "            #for i,pos in enumerate([fwd_pos, rev_pos]):\n",
    "            d_fwd = pd.DataFrame(fwd_pos, columns=['pos'])\n",
    "            d_fwd['s'] = d_fwd.apply(lambda row: \"\".join(seq[row.pos - (k_window+k-1): row.pos + (k_window+k)]), axis=1).to_frame()\n",
    "            d_fwd = d_fwd.reindex(d_fwd.columns.tolist() + all_kmers, axis=1, fill_value=0)\n",
    "            d_fwd = d_fwd.parallel_apply(kmers, n=n, k=k, axis=1).set_index('pos').drop('s', axis=1)\n",
    "            d_rev = pd.DataFrame(rev_pos, columns=['pos'])\n",
    "            d_rev['s'] = d_rev.apply(lambda row: \"\".join(seq[row.pos - (2+k-1): row.pos + (2+k)]), axis=1).to_frame()\n",
    "            d_rev = d_rev.reindex(d_rev.columns.tolist() + all_kmers, axis=1, fill_value=0)\n",
    "            d_rev = d_rev.parallel_apply(kmers, n=n, k=k, axis=1).set_index('pos').drop('s', axis=1)\n",
    "            d = d_fwd.sum() + d_rev.sum()\n",
    "            d = d - (kmer_frequencies[k].loc[d.index]['count'] * d.sum()).astype(int)\n",
    "            frequent_kmers = d[d > d.quantile(1 - kmer_quantile)].index\n",
    "            #print(d[frequent_kmers].sort_values())\n",
    "            fwd_pos = d_fwd[d_fwd[frequent_kmers].sum(axis=1) > 0].index.to_numpy()\n",
    "            rev_pos = d_rev[d_rev[frequent_kmers].sum(axis=1) > 0].index.to_numpy()\n",
    "    # de-select when too close to other hit\n",
    "    if min_dist > 1:\n",
    "        for i,pos in enumerate([fwd_pos, rev_pos]):\n",
    "            dists = pos[1:] - pos[:-1]\n",
    "            if dist_rem == 'all':\n",
    "                dists = np.pad(dists, (1,0), constant_values=0)\n",
    "                pos = pos[~((dists < min_dist) | (np.pad(dists[1:], (0,1), constant_values=0) < min_dist))]\n",
    "            elif dist_rem == 'middle':\n",
    "                while np.any(dists < min_dist):\n",
    "                    dists = np.pad(dists, 1, constant_values=[min_dist+1, min_dist+1])#[1,0])\n",
    "                    diff = np.diff((dists < min_dist).astype(int))\n",
    "                    clus = np.column_stack((np.where(diff == 1)[0], np.where(diff == -1)[0]))\n",
    "                    rem = clus[:,0] + 1 + (clus[:,1] - clus[:,0]) // 2 # remove center elem of a cluster of close ones\n",
    "                    pos = np.delete(pos, rem)\n",
    "                    dists = pos[1:] - pos[:-1]\n",
    "            if i == 0:\n",
    "                fwd_pos = pos\n",
    "            else:\n",
    "                rev_pos = pos\n",
    "    # de-select non-CA\n",
    "    if only_CA:\n",
    "        fwd_pos = fwd_pos[fwd_ca[fwd_pos] == True]\n",
    "        rev_pos = rev_pos[rev_ca[rev_pos] == True]\n",
    "    # subsample randomly\n",
    "    if subsample < 1.:\n",
    "        fwd_pos = np.random.choice(fwd_pos, int(subsample*len(fwd_pos)), replace=False)\n",
    "        rev_pos = np.random.choice(rev_pos, int(subsample*len(rev_pos)), replace=False)\n",
    "    return fwd_pos, rev_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a14af9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dir = \"../samples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51991af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OC8',\n",
       " 'LA_Y3C',\n",
       " 'Bak13',\n",
       " 'Riv16',\n",
       " 'Red_Oak2',\n",
       " 'LM10',\n",
       " 'Riv19',\n",
       " 'Filmore',\n",
       " 'De_Donno',\n",
       " 'Bak8',\n",
       " 'Oak_35874',\n",
       " 'Temecula_1',\n",
       " 'Riv25']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = [d for d in os.listdir(sample_dir)]\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36c3c9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing sample OC8\n",
      "OC8_peaks_uBH_0.001.fasta 9063 8905\n",
      "OC8_peaks_uBH_0.001_peak_dist_2_min_cov_20_min_dist_20.fasta 5621 5400\n",
      "OC8_peaks_uBH_0.001_peak_dist_2_min_cov_20_min_dist_20_k_3_kmer_quantile_0.25.fasta 4967 4769\n",
      "processing sample LA_Y3C\n",
      "LA_Y3C_peaks_uBH_0.001.fasta 7717 7773\n",
      "LA_Y3C_peaks_uBH_0.001_peak_dist_2_min_cov_20_min_dist_20.fasta 4686 4679\n",
      "LA_Y3C_peaks_uBH_0.001_peak_dist_2_min_cov_20_min_dist_20_k_3_kmer_quantile_0.25.fasta 3904 4017\n",
      "processing sample Bak13\n",
      "Bak13_peaks_uBH_0.001.fasta 8810 9013\n",
      "Bak13_peaks_uBH_0.001_peak_dist_2_min_cov_20_min_dist_20.fasta 5392 5522\n",
      "Bak13_peaks_uBH_0.001_peak_dist_2_min_cov_20_min_dist_20_k_3_kmer_quantile_0.25.fasta 4773 4964\n",
      "processing sample Riv16\n",
      "Riv16_peaks_uBH_0.001.fasta 2814 2271\n",
      "Riv16_peaks_uBH_0.001_peak_dist_2_min_cov_20_min_dist_20.fasta 1896 1465\n",
      "Riv16_peaks_uBH_0.001_peak_dist_2_min_cov_20_min_dist_20_k_3_kmer_quantile_0.25.fasta 1670 1283\n",
      "processing sample Red_Oak2\n",
      "Red_Oak2_peaks_uBH_0.001.fasta 8872 8161\n",
      "Red_Oak2_peaks_uBH_0.001_peak_dist_2_min_cov_20_min_dist_20.fasta 5398 4862\n",
      "Red_Oak2_peaks_uBH_0.001_peak_dist_2_min_cov_20_min_dist_20_k_3_kmer_quantile_0.25.fasta 4694 4200\n",
      "processing sample LM10\n",
      "LM10_peaks_uBH_0.001.fasta 6112 6060\n",
      "LM10_peaks_uBH_0.001_peak_dist_2_min_cov_20_min_dist_20.fasta 3482 3504\n",
      "LM10_peaks_uBH_0.001_peak_dist_2_min_cov_20_min_dist_20_k_3_kmer_quantile_0.25.fasta 2994 2941\n",
      "processing sample Riv19\n",
      "Riv19_peaks_uBH_0.001.fasta 2084 2030\n",
      "Riv19_peaks_uBH_0.001_peak_dist_2_min_cov_20_min_dist_20.fasta 1435 1362\n",
      "Riv19_peaks_uBH_0.001_peak_dist_2_min_cov_20_min_dist_20_k_3_kmer_quantile_0.25.fasta 1257 1193\n",
      "processing sample Filmore\n",
      "Filmore_peaks_uBH_0.001.fasta 8659 8864\n",
      "Filmore_peaks_uBH_0.001_peak_dist_2_min_cov_20_min_dist_20.fasta 5551 5639\n",
      "Filmore_peaks_uBH_0.001_peak_dist_2_min_cov_20_min_dist_20_k_3_kmer_quantile_0.25.fasta 4647 4831\n",
      "processing sample De_Donno\n",
      "De_Donno_peaks_uBH_0.001.fasta 12994 13001\n",
      "De_Donno_peaks_uBH_0.001_peak_dist_2_min_cov_20_min_dist_20.fasta 7760 7678\n",
      "De_Donno_peaks_uBH_0.001_peak_dist_2_min_cov_20_min_dist_20_k_3_kmer_quantile_0.25.fasta 6915 6875\n",
      "processing sample Bak8\n",
      "Bak8_peaks_uBH_0.001.fasta 5471 5320\n",
      "Bak8_peaks_uBH_0.001_peak_dist_2_min_cov_20_min_dist_20.fasta 3458 3296\n",
      "Bak8_peaks_uBH_0.001_peak_dist_2_min_cov_20_min_dist_20_k_3_kmer_quantile_0.25.fasta 3089 2977\n",
      "processing sample Oak_35874\n",
      "Oak_35874_peaks_uBH_0.001.fasta 5499 5042\n",
      "Oak_35874_peaks_uBH_0.001_peak_dist_2_min_cov_20_min_dist_20.fasta 3468 3335\n",
      "Oak_35874_peaks_uBH_0.001_peak_dist_2_min_cov_20_min_dist_20_k_3_kmer_quantile_0.25.fasta 2993 2893\n",
      "processing sample Temecula_1\n",
      "Temecula_1_peaks_uBH_0.001.fasta 16242 15984\n",
      "Temecula_1_peaks_uBH_0.001_peak_dist_2_min_cov_20_min_dist_20.fasta 8763 8674\n",
      "Temecula_1_peaks_uBH_0.001_peak_dist_2_min_cov_20_min_dist_20_k_3_kmer_quantile_0.25.fasta 7574 7387\n",
      "processing sample Riv25\n",
      "Riv25_peaks_uBH_0.001.fasta 6873 6944\n",
      "Riv25_peaks_uBH_0.001_peak_dist_2_min_cov_20_min_dist_20.fasta 4688 4645\n",
      "Riv25_peaks_uBH_0.001_peak_dist_2_min_cov_20_min_dist_20_k_3_kmer_quantile_0.25.fasta 3983 4021\n"
     ]
    }
   ],
   "source": [
    "context = 22\n",
    "for sample in samples:\n",
    "    print(\"processing sample\", sample)\n",
    "    # load data\n",
    "    fp = f\"../samples/{sample}/diff/{sample}_difference.RDS\"\n",
    "    df = pyreadr.read_r(fp)[None]\n",
    "    # correct positions\n",
    "    df['position'] = df['position'].astype(int) - 1 + 3 # convert to 0-based indexing\n",
    "    df.loc[df.dir == 'rev', 'position'] += 1\n",
    "    df = df.set_index(df.position)\n",
    "    # read genome sequence\n",
    "    fp = f\"../samples/{sample}/ref_genome/{sample}.fasta\"\n",
    "    seq = \"\"\n",
    "    contig = None\n",
    "    for record in SeqIO.parse(fp, \"fasta\"):\n",
    "        if len(record.seq) > len(seq):\n",
    "            seq = record.seq\n",
    "            contig = record.id\n",
    "    sa = suffix_array.get_suffix_array(contig, seq)\n",
    "    # coverage\n",
    "    fwd_cov = np.full(len(seq), np.nan)\n",
    "    sel = ((df.dir == 'fwd') & ~pd.isna(df.u_test_pval))\n",
    "    fwd_cov[df.loc[(df.dir == 'fwd') & ~pd.isna(df.u_test_pval), 'u_test_pval'].index] = np.min([df.loc[sel, 'N_wga'], df.loc[sel, 'N_nat']], axis=0)\n",
    "    rev_cov = np.full(len(seq), np.nan)\n",
    "    sel = ((df.dir == 'rev') & ~pd.isna(df.u_test_pval))\n",
    "    rev_cov[df.loc[(df.dir == 'rev') & ~pd.isna(df.u_test_pval), 'u_test_pval'].index] = np.min([df.loc[sel, 'N_wga'], df.loc[sel, 'N_nat']], axis=0)\n",
    "    # p-values\n",
    "    fwd_p = np.full(len(seq), np.nan)\n",
    "    fwd_p[df.loc[(df.dir == 'fwd') & ~pd.isna(df.u_test_pval), 'u_test_pval'].index] = df.loc[(df.dir == 'fwd') & ~pd.isna(df.u_test_pval), 'u_test_pval']\n",
    "    rev_p = np.full(len(seq), np.nan)\n",
    "    rev_p[df.loc[(df.dir == 'rev') & ~pd.isna(df.u_test_pval), 'u_test_pval'].index] = df.loc[(df.dir == 'rev') & ~pd.isna(df.u_test_pval), 'u_test_pval']\n",
    "    # bonferoni-hochberg corrected p-values\n",
    "    _,corr_p_values,_,_ = multipletests(np.concatenate([fwd_p[~np.isnan(fwd_p)], rev_p[~np.isnan(rev_p)]]), alpha=0.05, method='fdr_bh')\n",
    "    fwd_p_corr = fwd_p.copy()\n",
    "    fwd_p_corr[~np.isnan(fwd_p)] = corr_p_values[:(~np.isnan(fwd_p)).sum()]\n",
    "    rev_p_corr = rev_p.copy()\n",
    "    rev_p_corr[~np.isnan(rev_p)] = corr_p_values[(~np.isnan(fwd_p)).sum():]\n",
    "    # calculate kmer frequencies in genome\n",
    "    kmer_frequencies = {}\n",
    "    for k in [3,4]:\n",
    "        kmer_frequencies[k] = get_kmer_frequencies(seq, k)\n",
    "    \n",
    "    thr = 0.001\n",
    "    for kwargs in [{},\n",
    "                   {'peak_dist':2, 'min_cov':20, 'min_dist':20},\n",
    "                   {'peak_dist':2, 'min_cov':20, 'min_dist':20, 'k':3, 'kmer_quantile':0.25}\n",
    "                  ]:\n",
    "        fwd_pos, rev_pos = select_positions(fwd_p_corr, rev_p_corr, thr, **kwargs)\n",
    "        fn = \"\".join([f\"{sample}_peaks_uBH_{thr}\"] + [f\"_{kw}_{val}\" for kw,val in kwargs.items()]) + \".fasta\"\n",
    "        print(fn, len(fwd_pos), len(rev_pos))\n",
    "        fp = os.path.join(sample_dir, sample, \"peaks\")\n",
    "        os.makedirs(fp, exist_ok=True)\n",
    "        fp = os.path.join(fp, fn)\n",
    "        with open(fp, \"w\") as f:\n",
    "            for pos in fwd_pos:\n",
    "                print(f\">{contig}_fwd_{pos}\\n{seq[pos-context:pos+context+1].upper()}\", file=f)\n",
    "            for pos in rev_pos:\n",
    "                print(f\">{contig}_rev_{pos}\\n{reverse_complement(str(seq[pos-context:pos+context+1].upper()))}\", file=f)\n",
    "    suffix_array.delete_suffix_array(sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6aeef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
